"""
Scraper detallado de OfferUp - Entra a cada producto individual
"""
import re
import time
import logging
import os
import signal
import sys
import smtplib
import getpass
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders
from datetime import datetime
from time import perf_counter
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from scraper import WebScraper
from utils import save_to_json, save_to_csv, clean_text
from config import Config

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('offerup_detailed.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Diccionario para tracking de tiempos
timing_stats = {}

# Variable global para manejar interrupci√≥n
interrupted = False

def signal_handler(sig, frame):
    """Manejador para Ctrl+C - guarda datos antes de salir"""
    global interrupted
    interrupted = True
    logger.warning("\n\n‚ö†Ô∏è  Interrupci√≥n detectada (Ctrl+C)")
    logger.info("Finalizando de forma segura y guardando datos recolectados...")

def log_timing(step_name, start_time):
    """Helper para logear tiempo de ejecuci√≥n de cada paso"""
    elapsed = time.perf_counter() - start_time
    timing_stats[step_name] = elapsed
    logger.info(f"‚è±Ô∏è  {step_name}: {elapsed:.2f}s")
    return elapsed

def print_timing_summary():
    """Imprime un resumen organizado de todos los tiempos"""
    logger.info("\n" + "="*70)
    logger.info("üìä RESUMEN DETALLADO DE TIEMPOS POR OPERACI√ìN")
    logger.info("="*70)
    
    # Agrupar por categor√≠as
    categories = {
        "Setup Inicial": [],
        "Configuraci√≥n": [],
        "B√∫squeda y Filtros": [],
        "Obtenci√≥n de Enlaces": [],
        "Extracci√≥n de Productos": [],
        "Navegaci√≥n de P√°ginas": [],
        "Total": []
    }
    
    for key, value in sorted(timing_stats.items(), key=lambda x: x[1], reverse=True):
        if "Navegaci√≥n inicial" in key:
            categories["Setup Inicial"].append((key, value))
        elif "Configuraci√≥n de ubicaci√≥n" in key:
            categories["Configuraci√≥n"].append((key, value))
        elif "B√∫squeda" in key or "filtros" in key:
            categories["B√∫squeda y Filtros"].append((key, value))
        elif "enlaces" in key:
            categories["Obtenci√≥n de Enlaces"].append((key, value))
        elif "Producto" in key or "‚îî‚îÄ" in key:
            categories["Extracci√≥n de Productos"].append((key, value))
        elif "P√°gina" in key:
            categories["Navegaci√≥n de P√°ginas"].append((key, value))
        elif "TOTAL" in key:
            categories["Total"].append((key, value))
    
    for category, items in categories.items():
        if items:
            logger.info(f"\nüìÇ {category}:")
            for name, duration in sorted(items, key=lambda x: x[1], reverse=True):
                logger.info(f"  {name}: {duration:.2f}s")
    
    logger.info("\n" + "="*70 + "\n")


class OfferUpDetailedScraper:
    """Scraper que entra a cada producto de OfferUp"""
    
    def __init__(self, headless=False):
        self.scraper = WebScraper(headless=headless, timeout=15)
        self.base_url = "https://offerup.com/"
        self.all_products = []
    
    def configure_location(self, zip_code: str = "92101"):
        """
        Configura la ubicaci√≥n en OfferUp usando c√≥digo postal (basado en recorded_actions.json)
        
        Args:
            zip_code: C√≥digo postal (ej: "92101" para San Diego)
        """
        logger.info(f"Configurando ubicaci√≥n con c√≥digo postal: {zip_code}")
        try:
            time.sleep(3)
            
            # PASO 1: Click en el elemento de ubicaci√≥n actual (Santa Monica:, etc)
            # Buscar SPAN con clase MuiTypography-subtitle1 que contenga ":"
            location_spans = self.scraper.driver.find_elements(By.XPATH, 
                "//span[contains(@class, 'MuiTypography-subtitle1') and contains(text(), ':')]")
            
            clicked_location = False
            for elem in location_spans:
                try:
                    if elem.is_displayed():
                        logger.info(f"Ubicaci√≥n actual detectada: {elem.text}")
                        self.scraper.driver.execute_script("arguments[0].click();", elem)
                        time.sleep(3)
                        clicked_location = True
                        logger.info("‚úì PASO 1: Clic en ubicaci√≥n actual")
                        break
                except:
                    continue
            
            if not clicked_location:
                logger.warning("No se pudo hacer clic en la ubicaci√≥n, intentando alternativa...")
                return False
            
            # PASO 2: Click en la ubicaci√≥n mostrada en el modal (Santa Monica, CA 90403)
            time.sleep(2)
            current_location_p = self.scraper.driver.find_elements(By.XPATH, 
                "//p[contains(@class, 'MuiTypography-body1')]")
            
            clicked_current = False
            for elem in current_location_p:
                try:
                    text = elem.text
                    if elem.is_displayed() and ('CA' in text or len(text) > 10):
                        logger.info(f"Haciendo clic en: {text}")
                        self.scraper.driver.execute_script("arguments[0].click();", elem)
                        time.sleep(3)
                        clicked_current = True
                        logger.info("‚úì PASO 2: Clic en ubicaci√≥n actual en modal")
                        break
                except:
                    continue
            
            # PASO 3: Buscar y hacer clic en el campo INPUT para c√≥digo postal
            time.sleep(2)
            all_inputs = self.scraper.driver.find_elements(By.TAG_NAME, "input")
            zip_input = None
            
            for inp in all_inputs:
                try:
                    if inp.is_displayed():
                        # Buscar input con clase MuiInputBase-input jss232 jss1102
                        classes = inp.get_attribute('class') or ''
                        if 'MuiInputBase-input' in classes and inp.get_attribute('type') != 'search':
                            zip_input = inp
                            logger.info(f"‚úì PASO 3: Campo de c√≥digo postal encontrado")
                            break
                except:
                    continue
            
            if zip_input:
                # PASO 3: Click en el campo
                zip_input.click()
                time.sleep(1)
                
                # Borrar TODO el contenido del input (CTRL+A y DELETE)
                zip_input.send_keys(Keys.CONTROL + "a")
                time.sleep(0.3)
                zip_input.send_keys(Keys.DELETE)
                time.sleep(0.3)
                
                # Asegurar que est√° vac√≠o usando JavaScript tambi√©n
                self.scraper.driver.execute_script("arguments[0].value = '';", zip_input)
                time.sleep(0.5)
                logger.info("‚úì Campo de c√≥digo postal limpiado")
                
                # PASO 4: Escribir c√≥digo postal car√°cter por car√°cter
                for char in zip_code:
                    zip_input.send_keys(char)
                    time.sleep(0.15)
                
                logger.info(f"‚úì PASO 4: C√≥digo postal escrito: {zip_input.get_attribute('value')}")
                time.sleep(2)
                
                # PASO 5: Click en bot√≥n "Apply"
                apply_buttons = self.scraper.driver.find_elements(By.XPATH, 
                    "//span[contains(@class, 'MuiTypography') and text()='Apply']")
                
                for btn in apply_buttons:
                    try:
                        if btn.is_displayed():
                            self.scraper.driver.execute_script("arguments[0].click();", btn)
                            time.sleep(3)
                            logger.info("‚úì PASO 5: Clic en Apply")
                            break
                    except:
                        continue
                
                # PASO 6: Click en "See listings"
                time.sleep(2)
                see_listings_buttons = self.scraper.driver.find_elements(By.XPATH, 
                    "//span[contains(@class, 'MuiTypography') and text()='See listings']")
                
                for btn in see_listings_buttons:
                    try:
                        if btn.is_displayed():
                            self.scraper.driver.execute_script("arguments[0].click();", btn)
                            time.sleep(3)
                            logger.info("‚úì PASO 6: Clic en See listings")
                            logger.info(f"‚úì‚úì‚úì Ubicaci√≥n configurada exitosamente: {zip_code}")
                            return True
                    except:
                        continue
                        
            logger.warning("No se pudo completar la configuraci√≥n de ubicaci√≥n")
            return False
                
        except Exception as e:
            logger.error(f"Error configurando ubicaci√≥n: {e}")
            return False
    
    def apply_price_filters(self, min_price: int, max_price: int):
        """
        Aplica filtros de precio
        
        Args:
            min_price: Precio m√≠nimo
            max_price: Precio m√°ximo
        """
        logger.info(f"Aplicando filtros de precio: ${min_price} - ${max_price}")
        try:
            time.sleep(3)
            
            # Scroll hacia arriba para asegurar que los filtros est√°n visibles
            self.scraper.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)
            
            # Buscar TODOS los inputs en la p√°gina
            all_inputs = self.scraper.driver.find_elements(By.TAG_NAME, "input")
            
            # Filtrar solo los que son de tipo text y est√°n visibles
            text_inputs = []
            for inp in all_inputs:
                try:
                    if inp.is_displayed() and inp.get_attribute('type') in ['text', 'number', None]:
                        # Verificar si es un campo de precio (suelen tener $ o n√∫meros)
                        placeholder = inp.get_attribute('placeholder') or ''
                        aria_label = inp.get_attribute('aria-label') or ''
                        if '$' in placeholder or 'price' in placeholder.lower() or 'price' in aria_label.lower():
                            text_inputs.append(inp)
                        elif len(text_inputs) < 2 and inp.size['height'] > 0:  # Campo visible
                            text_inputs.append(inp)
                except:
                    continue
            
            logger.info(f"Encontrados {len(text_inputs)} campos de texto visibles")
            
            if len(text_inputs) >= 2:
                try:
                    # Hacer scroll al elemento para asegurarse que est√° visible
                    self.scraper.driver.execute_script("arguments[0].scrollIntoView(true);", text_inputs[0])
                    time.sleep(0.5)
                    
                    # Precio m√≠nimo - usar JavaScript para mayor confiabilidad
                    logger.info(f"Escribiendo precio m√≠nimo: {min_price}")
                    text_inputs[0].click()
                    time.sleep(0.5)
                    text_inputs[0].clear()
                    time.sleep(0.3)
                    # Escribir car√°cter por car√°cter con delay
                    for char in str(min_price):
                        text_inputs[0].send_keys(char)
                        time.sleep(0.1)
                    logger.info(f"‚úì Precio m√≠nimo: ${min_price}")
                    time.sleep(1)
                    
                    # Precio m√°ximo - escribir MUY despacio
                    self.scraper.driver.execute_script("arguments[0].scrollIntoView(true);", text_inputs[1])
                    time.sleep(0.5)
                    logger.info(f"Escribiendo precio m√°ximo: {max_price}")
                    text_inputs[1].click()
                    time.sleep(0.5)
                    text_inputs[1].clear()
                    time.sleep(0.3)
                    # Escribir car√°cter por car√°cter con delay mayor
                    for char in str(max_price):
                        text_inputs[1].send_keys(char)
                        time.sleep(0.15)  # Delay m√°s largo entre caracteres
                    
                    # Verificar que se escribi√≥ correctamente
                    actual_value = text_inputs[1].get_attribute('value')
                    logger.info(f"Valor escrito en campo m√°ximo: '{actual_value}'")
                    
                    # Si no se escribi√≥ completo, intentar con JavaScript
                    if actual_value != str(max_price):
                        logger.warning(f"Valor incorrecto, reintentando con JavaScript...")
                        self.scraper.driver.execute_script(f"arguments[0].value = '{max_price}';", text_inputs[1])
                        time.sleep(0.3)
                        actual_value = text_inputs[1].get_attribute('value')
                        logger.info(f"Nuevo valor: '{actual_value}'")
                    
                    logger.info(f"‚úì Precio m√°ximo: ${max_price}")
                    time.sleep(1.5)
                    
                    # Buscar bot√≥n Go usando m√∫ltiples estrategias
                    go_found = False
                    
                    # Estrategia 1: XPath con texto
                    try:
                        go_btn = self.scraper.driver.find_element(By.XPATH, "//button[contains(., 'Go')]")
                        if go_btn.is_displayed():
                            self.scraper.driver.execute_script("arguments[0].click();", go_btn)
                            logger.info("‚úì Filtros aplicados (bot√≥n Go - XPath)")
                            go_found = True
                            time.sleep(4)
                    except:
                        pass
                    
                    # Estrategia 2: Buscar todos los botones y verificar texto
                    if not go_found:
                        try:
                            all_buttons = self.scraper.driver.find_elements(By.TAG_NAME, "button")
                            for btn in all_buttons:
                                if btn.text.strip().lower() == 'go' and btn.is_displayed():
                                    self.scraper.driver.execute_script("arguments[0].click();", btn)
                                    logger.info("‚úì Filtros aplicados (bot√≥n Go - JavaScript)")
                                    go_found = True
                                    time.sleep(4)
                                    break
                        except:
                            pass
                    
                    # Estrategia 3: Presionar Enter si no se encontr√≥ el bot√≥n
                    if not go_found:
                        text_inputs[1].send_keys(Keys.RETURN)
                        logger.info("‚úì Filtros aplicados (Enter)")
                        time.sleep(4)
                        
                except Exception as e:
                    logger.error(f"Error interactuando con campos: {e}")
            else:
                logger.warning(f"Solo se encontraron {len(text_inputs)} campos, se necesitan al menos 2")
                
        except Exception as e:
            logger.error(f"Error aplicando filtros: {e}")
    
    def get_product_links(self, max_items=20):
        """
        Obtiene los enlaces de productos de la p√°gina actual
        
        Args:
            max_items: M√°ximo de items a obtener
            
        Returns:
            Lista de URLs de productos
        """
        logger.info("Obteniendo enlaces de productos...")
        product_links = []
        
        try:
            time.sleep(1.5)  # Reducido de 3s a 1.5s
            
            # Scroll para cargar m√°s items
            for _ in range(2):
                self.scraper.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)  # Reducido de 2s a 1s
            
            # Buscar enlaces a productos
            link_selectors = [
                "a[href*='/item/']",
                "a[href*='/detail/']"
            ]
            
            links = []
            for selector in link_selectors:
                links = self.scraper.driver.find_elements(By.CSS_SELECTOR, selector)
                if links:
                    logger.info(f"Encontrados {len(links)} enlaces con selector: {selector}")
                    break
            
            # Extraer URLs √∫nicas
            seen_urls = set()
            for link in links:
                try:
                    url = link.get_attribute('href')
                    if url and '/item/' in url and url not in seen_urls:
                        product_links.append(url)
                        seen_urls.add(url)
                        if len(product_links) >= max_items:
                            break
                except:
                    continue
            
            logger.info(f"‚úì Se obtuvieron {len(product_links)} enlaces √∫nicos")
            
        except Exception as e:
            logger.error(f"Error obteniendo enlaces: {e}")
        
        return product_links[:max_items]
    
    def extract_product_details(self, product_url: str, index: int):
        """
        Entra a un producto individual y extrae toda la informaci√≥n
        
        Args:
            product_url: URL del producto
            index: √çndice del producto
            
        Returns:
            Diccionario con datos del producto
        """
        logger.info(f"\n[{index}] Entrando a producto: {product_url}")
        
        product_data = {
            "index": index,
            "url": product_url,
            "title": "",
            "price": "",
            "price_value": None,
            "description": "",
            "condition": "",
            "location": "",
            "seller_name": "",
            "posted_date": "",
            "images": [],
            "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        try:
            # Navegar al producto
            nav_start = time.perf_counter()
            self.scraper.driver.get(product_url)
            time.sleep(2)
            log_timing(f"      ‚îî‚îÄ Navegaci√≥n a producto", nav_start)
            
            # Obtener todo el texto de la p√°gina para extraer informaci√≥n
            text_start = time.perf_counter()
            page_text = self.scraper.driver.find_element(By.TAG_NAME, "body").text
            log_timing(f"      ‚îî‚îÄ Obtenci√≥n de texto de p√°gina", text_start)
            
            # T√≠tulo - Usar el t√≠tulo de la p√°gina como fallback
            title_start = time.perf_counter()
            try:
                page_title = self.scraper.driver.title
                if page_title and page_title != "OfferUp":
                    product_data["title"] = clean_text(page_title.split('-')[0].strip())
                    logger.info(f"  T√≠tulo: {product_data['title'][:50]}...")
            except:
                pass
            
            # Buscar t√≠tulo en la p√°gina
            title_selectors = ["h1", "h2", "[data-testid='item-title']"]
            for selector in title_selectors:
                try:
                    title_elem = self.scraper.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_elem and title_elem.text and len(title_elem.text) > 3:
                        product_data["title"] = clean_text(title_elem.text)
                        logger.info(f"  T√≠tulo: {product_data['title'][:50]}...")
                        break
                except:
                    continue
            log_timing(f"      ‚îî‚îÄ Extracci√≥n de t√≠tulo", title_start)
            
            # Precio - buscar en el texto de la p√°gina
            price_start = time.perf_counter()
            price_pattern = r'\$[\d,]+(?:\.\d{2})?'
            prices_found = re.findall(price_pattern, page_text)
            if prices_found:
                product_data["price"] = prices_found[0]  # Tomar el primer precio encontrado
                try:
                    price_match = re.search(r'[\d,]+', product_data["price"].replace('$', ''))
                    if price_match:
                        product_data["price_value"] = int(price_match.group().replace(',', ''))
                    logger.info(f"  Precio: {product_data['price']}")
                except:
                    pass
            log_timing(f"      ‚îî‚îÄ Extracci√≥n de precio", price_start)
            
            # Descripci√≥n - buscar en m√∫ltiples lugares con timeout corto
            desc_start = time.perf_counter()
            desc_selectors = [
                "[data-testid='item-description']",
                "div[class*='description']",
                "p[class*='description']",
                "pre"
            ]
            # Reducir temporalmente el implicit wait para descripci√≥n
            original_timeout = self.scraper.driver.timeouts.implicit_wait
            self.scraper.driver.implicitly_wait(1)  # Solo 1 segundo para descripci√≥n
            
            for selector in desc_selectors:
                try:
                    desc_elem = self.scraper.driver.find_element(By.CSS_SELECTOR, selector)
                    if desc_elem and desc_elem.text and len(desc_elem.text) > 20:
                        product_data["description"] = clean_text(desc_elem.text)[:500]
                        logger.info(f"  Descripci√≥n: {len(product_data['description'])} caracteres")
                        break
                except:
                    continue
            
            # Restaurar timeout original
            self.scraper.driver.implicitly_wait(original_timeout)
            log_timing(f"      ‚îî‚îÄ Extracci√≥n de descripci√≥n", desc_start)
            
            # Extraer ubicaci√≥n del texto
            location_start = time.perf_counter()
            if "San Diego" in page_text or "CA" in page_text:
                # Buscar patr√≥n de ubicaci√≥n
                location_match = re.search(r'([A-Z][a-z]+(?:\s[A-Z][a-z]+)*,\s*[A-Z]{2})', page_text)
                if location_match:
                    product_data["location"] = location_match.group(1)
            log_timing(f"      ‚îî‚îÄ Extracci√≥n de ubicaci√≥n", location_start)
            
            # Im√°genes - extracci√≥n inmediata sin delay
            try:
                img_start = time.perf_counter()
                img_elements = self.scraper.driver.find_elements(By.CSS_SELECTOR, "img")
                for img in img_elements[:5]:
                    src = img.get_attribute('src')
                    if src and ('offerup' in src or 'cloudfront' in src) and src.startswith('http'):
                        product_data["images"].append(src)
                if product_data["images"]:
                    logger.info(f"  Im√°genes: {len(product_data['images'])} encontradas")
                log_timing(f"      ‚îî‚îÄ Extracci√≥n de im√°genes", img_start)
            except:
                pass
            
            logger.info(f"‚úì Producto {index} extra√≠do exitosamente")
            
        except Exception as e:
            logger.error(f"Error extrayendo producto {index}: {e}")
        
        return product_data
    
    def scrape_with_pagination(self, search_term: str, location: str, min_price: int, max_price: int, 
                                max_items: int = 100):
        """
        Scraping completo con paginaci√≥n din√°mica
        
        Args:
            search_term: T√©rmino de b√∫squeda
            location: Ubicaci√≥n
            min_price: Precio m√≠nimo
            max_price: Precio m√°ximo
            max_items: Total de items a extraer (default: 100)
        """
        logger.info("\n" + "="*60)
        logger.info("INICIANDO SCRAPING DETALLADO DE OFFERUP")
        logger.info("="*60)
        logger.info(f"B√∫squeda: {search_term}")
        logger.info(f"Ubicaci√≥n: {location}")
        logger.info(f"Precio: ${min_price} - ${max_price}")
        logger.info(f"Total de items a extraer: {max_items}")
        logger.info("="*60 + "\n")
        
        scraping_start = time.perf_counter()
        
        try:
            self.scraper.setup_driver()
            
            # 1. Navegar a OfferUp
            step_start = time.perf_counter()
            logger.info("Paso 1: Navegando a OfferUp...")
            self.scraper.get_page(self.base_url)
            logger.info("‚è≥ Esperando 20 segundos para que cargue completamente...")
            time.sleep(20)
            log_timing("1. Navegaci√≥n inicial + carga", step_start)
            
            # 2. Configurar ubicaci√≥n PRIMERO (antes de buscar)
            step_start = time.perf_counter()
            logger.info("Paso 2: Configurando ubicaci√≥n...")
            self.configure_location(location)
            log_timing("2. Configuraci√≥n de ubicaci√≥n", step_start)
            
            # 3. Buscar producto
            step_start = time.perf_counter()
            logger.info("Paso 3: Buscando '{search_term}'...")
            search_box = self.scraper.wait_for_element(By.CSS_SELECTOR, "input[type='search'], input[placeholder*='Search']")
            if search_box:
                search_box.clear()
                search_box.send_keys(search_term)
                search_box.send_keys(Keys.RETURN)
                time.sleep(5)
                logger.info("‚úì B√∫squeda realizada")
            log_timing("3. B√∫squeda de producto", step_start)
            
            # 4. Aplicar filtros de precio
            step_start = time.perf_counter()
            logger.info("Paso 4: Aplicando filtros de precio...")
            self.apply_price_filters(min_price, max_price)
            log_timing("4. Aplicaci√≥n de filtros", step_start)
            
            # 5. Procesar p√°ginas din√°micamente
            page_num = 1
            total_extracted = 0
            
            while total_extracted < max_items:
                # Verificar si hay interrupci√≥n
                if interrupted:
                    logger.warning("‚ö†Ô∏è  Deteniendo scraping por interrupci√≥n del usuario...")
                    break
                
                page_start = time.perf_counter()
                logger.info(f"\n{'='*60}")
                logger.info(f"P√ÅGINA {page_num} - Extra√≠dos: {total_extracted}/{max_items}")
                logger.info(f"{'='*60}\n")
                
                # Obtener TODOS los enlaces de productos de la p√°gina actual
                links_start = time.perf_counter()
                product_links = self.get_product_links(max_items=999)
                log_timing(f"5.{page_num}.a Obtenci√≥n de enlaces", links_start)
                
                if not product_links:
                    logger.warning(f"No se encontraron productos en p√°gina {page_num}")
                    break
                
                # Calcular cu√°ntos productos procesar de esta p√°gina
                remaining = max_items - total_extracted
                items_to_process = min(len(product_links), remaining)
                
                logger.info(f"Items encontrados en p√°gina: {len(product_links)}")
                logger.info(f"Items a procesar: {items_to_process}\n")
                
                # Procesar cada producto
                products_start = time.perf_counter()
                for idx in range(items_to_process):
                    # Verificar interrupci√≥n en cada producto
                    if interrupted:
                        logger.warning("‚ö†Ô∏è  Deteniendo procesamiento de productos...")
                        break
                        
                    item_start = time.perf_counter()
                    product_url = product_links[idx]
                    global_index = total_extracted + idx + 1
                    product_data = self.extract_product_details(product_url, global_index)
                    self.all_products.append(product_data)
                    log_timing(f"   Producto {global_index}", item_start)
                    
                    # Volver a la p√°gina de resultados
                    if not interrupted:  # Solo volver si no fue interrumpido
                        self.scraper.driver.back()
                        time.sleep(1)
                
                # Si hubo interrupci√≥n durante procesamiento, actualizar contador con lo procesado
                if interrupted:
                    actual_processed = len([p for p in self.all_products if p['index'] > total_extracted])
                    total_extracted += actual_processed
                    break
                
                log_timing(f"5.{page_num}.b Procesamiento de {items_to_process} productos", products_start)
                
                # Actualizar contador total
                total_extracted += items_to_process
                log_timing(f"5.{page_num} P√°gina completa", page_start)
                logger.info(f"\n‚úì Total extra√≠do hasta ahora: {total_extracted}/{max_items}")
                
                # Si ya alcanzamos el m√°ximo, terminar
                if total_extracted >= max_items:
                    logger.info(f"\n‚úì‚úì‚úì Se alcanz√≥ el l√≠mite de {max_items} items")
                    break
                
                # Intentar ir a la siguiente p√°gina
                page_num += 1
                logger.info(f"\nIntentando ir a p√°gina {page_num}...")
                try:
                    # Buscar bot√≥n de siguiente p√°gina
                    next_buttons = [
                        "button[aria-label*='next']",
                        "a[aria-label*='next']",
                        "button:has-text('Next')",
                        "a:has-text('Next')"
                    ]
                    
                    next_clicked = False
                    for selector in next_buttons:
                        try:
                            next_btn = self.scraper.driver.find_element(By.CSS_SELECTOR, selector)
                            if next_btn and next_btn.is_displayed():
                                next_btn.click()
                                time.sleep(2)
                                next_clicked = True
                                logger.info(f"‚úì Navegando a p√°gina {page_num}")
                                break
                        except:
                            continue
                    
                    if not next_clicked:
                        logger.info("No hay m√°s p√°ginas disponibles")
                        break
                        
                except Exception as e:
                    logger.warning(f"Error al cambiar de p√°gina: {e}")
                    break
            
        except KeyboardInterrupt:
            logger.warning("\n‚ö†Ô∏è  Interrupci√≥n por teclado (Ctrl+C)")
            logger.info("Guardando datos recolectados antes de salir...")
        
        except Exception as e:
            logger.error(f"Error durante el scraping: {e}")
        
        finally:
            total_time = time.perf_counter() - scraping_start
            log_timing("TOTAL SCRAPING", scraping_start)
            
            # Mostrar resumen detallado de tiempos por operaci√≥n
            print_timing_summary()
            
            if interrupted:
                logger.info(f"üíæ Datos recolectados antes de la interrupci√≥n: {len(self.all_products)} productos")
            
            self.scraper.close()
        
        return self.all_products


def generate_mobile_html(products, search_term, location, min_price, max_price):
    """Genera HTML optimizado para mobile con todos los productos"""
    
    # Ordenar productos por precio (de menor a mayor)
    def extract_price(product):
        price_str = product.get('price', '$0')
        # Extraer solo los n√∫meros del precio
        import re
        match = re.search(r'[\d,]+', price_str)
        if match:
            return float(match.group().replace(',', ''))
        return 0
    
    sorted_products = sorted(products, key=extract_price)
    
    html = f"""<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OfferUp - {search_term}</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #f5f5f5;
            color: #333;
            line-height: 1.6;
            padding-bottom: 20px;
        }}
        
        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px 15px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            position: sticky;
            top: 0;
            z-index: 100;
        }}
        
        .header h1 {{
            font-size: 24px;
            margin-bottom: 10px;
        }}
        
        .header .meta {{
            font-size: 14px;
            opacity: 0.9;
        }}
        
        .stats {{
            background: white;
            padding: 15px;
            margin: 15px;
            border-radius: 10px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 10px;
            margin-top: 10px;
        }}
        
        .stat-item {{
            text-align: center;
            padding: 10px;
            background: #f8f9fa;
            border-radius: 8px;
        }}
        
        .stat-label {{
            font-size: 12px;
            color: #666;
            margin-bottom: 5px;
        }}
        
        .stat-value {{
            font-size: 20px;
            font-weight: bold;
            color: #667eea;
        }}
        
        .container {{
            padding: 0 15px;
        }}
        
        .product-card {{
            background: white;
            border-radius: 12px;
            margin: 15px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            overflow: hidden;
            transition: transform 0.2s;
        }}
        
        .product-card:active {{
            transform: scale(0.98);
        }}
        
        .product-header {{
            position: relative;
            height: 250px;
            background: #e9ecef;
            overflow: hidden;
        }}
        
        .product-image {{
            width: 40%;
            height: 40%;
            object-fit: cover;
        }}
        
        .product-number {{
            position: absolute;
            top: 10px;
            left: 10px;
            background: rgba(0,0,0,0.7);
            color: white;
            padding: 5px 12px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: bold;
        }}
        
        .product-price {{
            position: absolute;
            bottom: 10px;
            right: 10px;
            background: #28a745;
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 20px;
            font-weight: bold;
            box-shadow: 0 2px 5px rgba(0,0,0,0.3);
        }}
        
        .product-content {{
            padding: 15px;
        }}
        
        .product-title {{
            font-size: 18px;
            font-weight: bold;
            margin-bottom: 10px;
            color: #2c3e50;
        }}
        
        .product-location {{
            color: #666;
            font-size: 14px;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 5px;
        }}
        
        .product-description {{
            color: #555;
            font-size: 14px;
            line-height: 1.6;
            margin-bottom: 15px;
            max-height: 100px;
            overflow: hidden;
            position: relative;
        }}
        
        .product-images {{
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            padding: 15px 0;
        }}
        
        .product-images::-webkit-scrollbar {{
            height: 8px;
        }}
        
        .product-images::-webkit-scrollbar-thumb {{
            background: #667eea;
            border-radius: 2px;
        }}
        
        .thumbnail {{
            width: 100%;
            height: auto;
            aspect-ratio: 1;
            border-radius: 12px;
            object-fit: cover;
            border: 3px solid #e9ecef;
        }}
        
        .product-link {{
            display: block;
            background: #667eea;
            color: white;
            text-align: center;
            padding: 12px;
            border-radius: 8px;
            text-decoration: none;
            font-weight: bold;
            margin-top: 10px;
        }}
        
        .product-link:active {{
            background: #5568d3;
        }}
        
        .footer {{
            text-align: center;
            padding: 20px;
            color: #666;
            font-size: 14px;
        }}
        
        @media (min-width: 768px) {{
            .container {{
                max-width: 600px;
                margin: 0 auto;
            }}
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üîç {search_term}</h1>
        <div class="meta">
            üìç {location} | üíµ ${min_price:,} - ${max_price:,}
        </div>
    </div>
    
    <div class="stats">
        <div class="stat-label">Resultados encontrados</div>
        <div class="stats-grid">
            <div class="stat-item">
                <div class="stat-label">Total</div>
                <div class="stat-value">{len(products)}</div>
            </div>
            <div class="stat-item">
                <div class="stat-label">Fecha</div>
                <div class="stat-value">{datetime.now().strftime('%d/%m/%Y')}</div>
            </div>
        </div>
    </div>
    
    <div class="container">
"""
    
    for idx, product in enumerate(sorted_products, 1):
        title = product.get('title', 'Sin t√≠tulo')
        price = product.get('price', 'N/A')
        location = product.get('location', 'Sin ubicaci√≥n')
        description = product.get('description', 'Sin descripci√≥n')
        images = product.get('images', [])
        url = product.get('url', '#')
        
        # Primera imagen como principal
        main_image = images[0] if images else 'https://via.placeholder.com/800x600?text=Sin+Imagen'
        
        html += f"""
        <div class="product-card">
            <div class="product-header">
                <img src="{main_image}" alt="{title}" class="product-image" onerror="this.src='https://via.placeholder.com/800x600?text=Sin+Imagen'">
                <div class="product-number">#{idx}</div>
                <div class="product-price">{price}</div>
            </div>
            <div class="product-content">
                <h2 class="product-title">{title}</h2>
                <div class="product-location">üìç {location}</div>
                <div class="product-description">{description[:200]}{'...' if len(description) > 200 else ''}</div>
"""
        
        # Thumbnails de im√°genes adicionales en grid de 2 columnas
        if len(images) > 1:
            html += '                <div class="product-images">\n'
            for img_url in images[1:6]:  # M√°ximo 5 thumbnails adicionales
                html += f'                    <img src="{img_url}" alt="Imagen" class="thumbnail" onerror="this.style.display=&apos;none&apos;">\n'
            html += '                </div>\n'
        
        html += f"""
                <a href="{url}" class="product-link" target="_blank">Ver en OfferUp ‚Üí</a>
            </div>
        </div>
"""
    
    html += f"""
    </div>
    
    <div class="footer">
        Generado el {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}<br>
        Total de productos: {len(products)}
    </div>
</body>
</html>
"""
    return html


def create_scheduled_task(task_name, script_path, schedule_time, config):
    """Crea una tarea programada en Windows"""
    try:
        import subprocess
        import json
        
        # Guardar configuraci√≥n en archivo JSON para la tarea programada
        config_file = os.path.join(os.path.dirname(script_path), 'scheduled_config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        # Crear script batch que ejecuta el scraper con la configuraci√≥n guardada
        batch_file = os.path.join(os.path.dirname(script_path), 'run_scheduled_scraper.bat')
        venv_python = os.path.join(os.path.dirname(script_path), 'venv', 'Scripts', 'python.exe')
        
        with open(batch_file, 'w') as f:
            f.write(f'@echo off\n')
            f.write(f'cd /d "{os.path.dirname(script_path)}"\n')
            f.write(f'"{venv_python}" "{script_path}" --scheduled\n')
        
        # Comando para crear tarea programada de Windows
        # Formato: schtasks /create /tn "nombre" /tr "comando" /sc DAILY /st HH:MM
        cmd = [
            'schtasks', '/create',
            '/tn', task_name,
            '/tr', f'"{batch_file}"',
            '/sc', 'DAILY',
            '/st', schedule_time,
            '/f'  # Fuerza la creaci√≥n incluso si ya existe
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            logger.info(f"‚úÖ Tarea programada creada: {task_name}")
            logger.info(f"‚è∞ Se ejecutar√° diariamente a las {schedule_time}")
            logger.info(f"üìù Configuraci√≥n guardada en: {config_file}")
            logger.info(f"\nüí° Para administrar tareas programadas:")
            logger.info(f"   - Ver: schtasks /query /tn \"{task_name}\"")
            logger.info(f"   - Eliminar: schtasks /delete /tn \"{task_name}\" /f")
            logger.info(f"   - O usa el Programador de tareas de Windows\n")
            return True
        else:
            logger.error(f"‚ùå Error al crear tarea programada: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Error al crear tarea programada: {e}")
        return False


def send_email_gmail(recipient_email, subject, html_content, html_file_path=None, sender_email=None, sender_password=None):
    """Env√≠a email con HTML usando Gmail SMTP"""
    try:
        # Usar credenciales de .env si no se proporcionaron
        if not sender_email:
            sender_email = Config.GMAIL_USER if Config.GMAIL_USER else input("\nüìß Email de Gmail (remitente): ").strip()
        
        if not sender_password:
            sender_password = Config.GMAIL_APP_PASSWORD if Config.GMAIL_APP_PASSWORD else None
            if not sender_password:
                print("\nüîë Contrase√±a de aplicaci√≥n de Gmail")
                print("   (Crear en: https://myaccount.google.com/apppasswords)")
                sender_password = getpass.getpass("   Password: ")
        
        # Crear mensaje
        msg = MIMEMultipart('alternative')
        msg['From'] = sender_email
        msg['To'] = recipient_email
        msg['Subject'] = subject
        
        # Adjuntar HTML como contenido
        html_part = MIMEText(html_content, 'html', 'utf-8')
        msg.attach(html_part)
        
        # Adjuntar archivo HTML si se especifica
        if html_file_path and os.path.exists(html_file_path):
            with open(html_file_path, 'rb') as f:
                attachment = MIMEBase('application', 'octet-stream')
                attachment.set_payload(f.read())
                encoders.encode_base64(attachment)
                attachment.add_header('Content-Disposition', f'attachment; filename="{os.path.basename(html_file_path)}"')
                msg.attach(attachment)
        
        # Conectar y enviar
        print("\nüì§ Conectando con Gmail...")
        server = smtplib.SMTP('smtp.gmail.com', 587)
        server.starttls()
        server.login(sender_email, sender_password)
        server.send_message(msg)
        server.quit()
        
        logger.info(f"‚úÖ Email enviado exitosamente a {recipient_email}")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Error al enviar email: {e}")
        return False


def get_user_input():
    """Solicita par√°metros de b√∫squeda al usuario"""

    print("\n" + "="*60)
    print("üîç CONFIGURACI√ìN DE B√öSQUEDA EN OFFERUP")
    print("="*60 + "\n")
    
    # T√©rmino de b√∫squeda
    while True:
        search_term = input("üìù T√©rmino de b√∫squeda (ej: iphone, ford bronco): ").strip()
        if search_term:
            break
        print("‚ùå Por favor ingresa un t√©rmino de b√∫squeda v√°lido\n")
    
    # C√≥digo postal
    while True:
        zip_code = input("\nüìç C√≥digo postal / ZIP Code (ej: 92101): ").strip()
        if zip_code and len(zip_code) == 5 and zip_code.isdigit():
            break
        print("‚ùå Por favor ingresa un c√≥digo postal v√°lido de 5 d√≠gitos\n")
    
    # Precio m√≠nimo
    while True:
        try:
            min_price_input = input("\nüíµ Precio m√≠nimo en USD (Enter para $0): ").strip()
            min_price = 0 if not min_price_input else int(min_price_input)
            if min_price >= 0:
                break
            print("‚ùå El precio m√≠nimo debe ser mayor o igual a 0\n")
        except ValueError:
            print("‚ùå Por favor ingresa un n√∫mero v√°lido\n")
    
    # Precio m√°ximo
    while True:
        try:
            max_price_input = input(f"üíµ Precio m√°ximo en USD (Enter para sin l√≠mite): ").strip()
            max_price = 999999 if not max_price_input else int(max_price_input)
            if max_price >= min_price:
                break
            print(f"‚ùå El precio m√°ximo debe ser mayor o igual al m√≠nimo (${min_price})\n")
        except ValueError:
            print("‚ùå Por favor ingresa un n√∫mero v√°lido\n")
    
    # Cantidad de items
    while True:
        try:
            max_items_input = input("\nüî¢ Cantidad de productos a extraer (Enter para 100): ").strip()
            max_items = 100 if not max_items_input else int(max_items_input)
            if max_items > 0:
                break
            print("‚ùå La cantidad debe ser mayor a 0\n")
        except ValueError:
            print("‚ùå Por favor ingresa un n√∫mero v√°lido\n")
    
    # Configuraci√≥n de email
    send_email = input("\nüìß ¬øEnviar resultados por email al finalizar? (S/n): ").strip().lower()
    send_email = send_email in ['s', 'si', 'yes', 'y', '']
    
    recipient_email = None
    if send_email:
        while True:
            recipient_email = input("üì© Email destinatario: ").strip()
            if recipient_email and '@' in recipient_email:
                break
            print("‚ùå Por favor ingresa un email v√°lido\n")
    
    # Configuraci√≥n de programaci√≥n diaria
    schedule_daily = input("\n‚è∞ ¬øProgramar esta b√∫squeda diariamente? (S/n): ").strip().lower()
    schedule_daily = schedule_daily in ['s', 'si', 'yes', 'y', '']
    
    schedule_time = None
    if schedule_daily:
        while True:
            schedule_time = input("üïê Hora de ejecuci√≥n diaria (formato 24h, ej: 14:30): ").strip()
            try:
                # Validar formato HH:MM
                hours, minutes = schedule_time.split(':')
                hours, minutes = int(hours), int(minutes)
                if 0 <= hours <= 23 and 0 <= minutes <= 59:
                    break
                print("‚ùå Hora inv√°lida. Usa formato 24h (00:00 - 23:59)\n")
            except:
                print("‚ùå Formato incorrecto. Usa HH:MM (ej: 14:30)\n")
    
    # Confirmaci√≥n
    print("\n" + "="*60)
    print("üìã RESUMEN DE CONFIGURACI√ìN:")
    print("="*60)
    print(f"üîç B√∫squeda: {search_term}")
    print(f"üìç Ubicaci√≥n: {zip_code}")
    print(f"üíµ Precio: ${min_price} - ${max_price}")
    print(f"üî¢ Cantidad: {max_items} productos")
    if send_email:
        print(f"üìß Email: {recipient_email}")
    if schedule_daily:
        print(f"‚è∞ Programaci√≥n: Diaria a las {schedule_time}")
    print("="*60)
    
    confirm = input("\n‚úÖ ¬øContinuar con esta configuraci√≥n? (S/n): ").strip().lower()
    if confirm and confirm not in ['s', 'si', 'yes', 'y', '']:
        print("\n‚ùå Operaci√≥n cancelada por el usuario")
        return None
    
    return {
        'search_term': search_term,
        'zip_code': zip_code,
        'min_price': min_price,
        'max_price': max_price,
        'max_items': max_items,
        'send_email': send_email,
        'recipient_email': recipient_email,
        'schedule_daily': schedule_daily,
        'schedule_time': schedule_time
    }


def main():
    """Funci√≥n principal"""
    import sys
    
    # Registrar manejador de se√±ales para Ctrl+C
    signal.signal(signal.SIGINT, signal_handler)
    
    Config.create_directories()
    
    # Verificar si se ejecuta desde tarea programada
    is_scheduled = '--scheduled' in sys.argv
    
    if is_scheduled:
        # Cargar configuraci√≥n guardada
        config_file = os.path.join(os.path.dirname(__file__), 'scheduled_config.json')
        if os.path.exists(config_file):
            import json
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("üìã Ejecutando tarea programada con configuraci√≥n guardada")
        else:
            logger.error("‚ùå No se encontr√≥ archivo de configuraci√≥n programada")
            return
    else:
        # Solicitar par√°metros al usuario
        config = get_user_input()
        if not config:
            return
    
    # Crear carpeta con timestamp para esta ejecuci√≥n
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_folder = os.path.join("data", f"scraping_{timestamp}")
    os.makedirs(output_folder, exist_ok=True)
    logger.info(f"üìÅ Carpeta de salida creada: {output_folder}")
    logger.info(f"‚ÑπÔ∏è  Presiona Ctrl+C en cualquier momento para detener y guardar datos\n")
    
    # Par√°metros del usuario
    search_term = config['search_term']
    zip_code = config['zip_code']
    min_price = config['min_price']
    max_price = config['max_price']
    max_items = config['max_items']
    
    # Crear scraper
    scraper = OfferUpDetailedScraper(headless=False)
    
    # Ejecutar scraping
    results = scraper.scrape_with_pagination(
        search_term=search_term,
        location=zip_code,  # C√≥digo postal
        min_price=min_price,
        max_price=max_price,
        max_items=max_items
    )
    
    # Guardar resultados en la carpeta con timestamp (siempre, incluso si fue interrumpido)
    if results:
        save_start = time.perf_counter()
        
        filename_json = os.path.join(output_folder, f"offerup_{search_term}_detailed.json")
        filename_csv = os.path.join(output_folder, f"offerup_{search_term}_detailed.csv")
        
        logger.info(f"\nüíæ Guardando {len(results)} productos...")
        save_to_json(results, filename_json)
        save_to_csv(results, filename_csv)
        
        # Generar HTML mobile-optimizado
        html_content = generate_mobile_html(results, search_term, zip_code, min_price, max_price)
        filename_html = os.path.join(output_folder, f"offerup_{search_term.replace(' ', '_')}_mobile.html")
        with open(filename_html, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"üì± HTML m√≥vil guardado: {filename_html}")
        
        save_time = time.perf_counter() - save_start
        
        logger.info("\n" + "="*60)
        if interrupted:
            logger.info("‚ö†Ô∏è  SCRAPING INTERRUMPIDO (datos guardados)")
        else:
            logger.info("‚úÖ SCRAPING COMPLETADO")
        logger.info("="*60)
        logger.info(f"Total de productos extra√≠dos: {len(results)}")
        logger.info(f"Carpeta de salida: {output_folder}")
        logger.info(f"Archivos guardados:")
        logger.info(f"  - {filename_json}")
        logger.info(f"  - {filename_csv}")
        logger.info(f"  - {filename_html}")
        logger.info(f"Tiempo de guardado: {save_time:.2f}s")
        logger.info("="*60 + "\n")
        
        # Enviar por email si fue configurado
        if not interrupted and config.get('send_email') and config.get('recipient_email'):
            subject = f"OfferUp - {search_term} ({len(results)} productos)"
            send_email_gmail(
                recipient_email=config['recipient_email'],
                subject=subject,
                html_content=html_content,
                html_file_path=filename_html
            )
    else:
        logger.warning("‚ö†Ô∏è  No se extrajeron productos")
    
    # Crear tarea programada si fue configurado (solo primera vez, no desde tarea programada)
    if not interrupted and not is_scheduled and config.get('schedule_daily') and config.get('schedule_time'):
        task_name = f"OfferUp_Scraper_{search_term.replace(' ', '_')}"
        script_path = os.path.abspath(__file__)
        create_scheduled_task(task_name, script_path, config['schedule_time'], config)


if __name__ == "__main__":
    main()
